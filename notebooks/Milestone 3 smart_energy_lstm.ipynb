{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xh6RUPJ29dVE"
      },
      "outputs": [],
      "source": [
        "#module 5 ADVANCED LSTM MODEL DEVELOPMENT\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 1: TARGET SMOOTHING + LOG TRANSFORMATION\n",
        "# ---------------------------------------------------\n",
        "df_features['Global_active_power_smooth'] = (\n",
        "    df_features['Global_active_power']\n",
        "    .rolling(window=3)\n",
        "    .mean()\n",
        "    .bfill()\n",
        ")\n",
        "\n",
        "df_features['Global_active_power_log'] = np.log1p(\n",
        "    df_features['Global_active_power_smooth']\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 2: FEATURE SELECTION (NO LAG FEATURES)\n",
        "# ---------------------------------------------------\n",
        "features = [\n",
        "    'Global_active_power_log',\n",
        "    'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3',\n",
        "    'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
        "    'is_weekend', 'is_peak'\n",
        "]\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 3: ROBUST SCALING\n",
        "# ---------------------------------------------------\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(df_features[features])\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 4: SEQUENCE CREATION (2 WEEKS CONTEXT)\n",
        "# ---------------------------------------------------\n",
        "def create_multivariate_sequences(data, seq_length=336):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length, 0])  # log target\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X_seq, y_seq = create_multivariate_sequences(scaled_data)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 5: TRAIN / TEST SPLIT\n",
        "# ---------------------------------------------------\n",
        "train_size = int(len(X_seq) * 0.8)\n",
        "X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
        "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENeZ-nSH9eZ0",
        "outputId": "1eaea0c2-035f-4a53-ed08-7084993d9a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 96ms/step - loss: 0.0817 - val_loss: 0.0236 - learning_rate: 0.0010\n",
            "Epoch 2/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 91ms/step - loss: 0.0252 - val_loss: 0.0174 - learning_rate: 0.0010\n",
            "Epoch 3/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 92ms/step - loss: 0.0194 - val_loss: 0.0149 - learning_rate: 0.0010\n",
            "Epoch 4/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 92ms/step - loss: 0.0175 - val_loss: 0.0143 - learning_rate: 0.0010\n",
            "Epoch 5/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 92ms/step - loss: 0.0159 - val_loss: 0.0136 - learning_rate: 0.0010\n",
            "Epoch 6/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 94ms/step - loss: 0.0148 - val_loss: 0.0124 - learning_rate: 0.0010\n",
            "Epoch 7/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 94ms/step - loss: 0.0140 - val_loss: 0.0122 - learning_rate: 0.0010\n",
            "Epoch 8/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 94ms/step - loss: 0.0134 - val_loss: 0.0118 - learning_rate: 0.0010\n",
            "Epoch 9/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 94ms/step - loss: 0.0135 - val_loss: 0.0115 - learning_rate: 0.0010\n",
            "Epoch 10/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0127 - val_loss: 0.0109 - learning_rate: 0.0010\n",
            "Epoch 11/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0125 - val_loss: 0.0104 - learning_rate: 0.0010\n",
            "Epoch 12/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - loss: 0.0121 - val_loss: 0.0115 - learning_rate: 0.0010\n",
            "Epoch 13/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0119 - val_loss: 0.0104 - learning_rate: 0.0010\n",
            "Epoch 14/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0116 - val_loss: 0.0098 - learning_rate: 0.0010\n",
            "Epoch 15/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0111 - val_loss: 0.0106 - learning_rate: 0.0010\n",
            "Epoch 16/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0109 - val_loss: 0.0102 - learning_rate: 0.0010\n",
            "Epoch 17/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0105 - val_loss: 0.0098 - learning_rate: 0.0010\n",
            "Epoch 18/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0104 - val_loss: 0.0092 - learning_rate: 0.0010\n",
            "Epoch 19/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0102 - val_loss: 0.0092 - learning_rate: 0.0010\n",
            "Epoch 20/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0100 - val_loss: 0.0088 - learning_rate: 0.0010\n",
            "Epoch 21/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0099 - val_loss: 0.0090 - learning_rate: 0.0010\n",
            "Epoch 22/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0097 - val_loss: 0.0109 - learning_rate: 0.0010\n",
            "Epoch 23/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0096 - val_loss: 0.0086 - learning_rate: 0.0010\n",
            "Epoch 24/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0093 - val_loss: 0.0087 - learning_rate: 0.0010\n",
            "Epoch 25/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0091 - val_loss: 0.0083 - learning_rate: 0.0010\n",
            "Epoch 26/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0089 - val_loss: 0.0087 - learning_rate: 0.0010\n",
            "Epoch 27/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - loss: 0.0089 - val_loss: 0.0085 - learning_rate: 0.0010\n",
            "Epoch 28/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0088 - val_loss: 0.0085 - learning_rate: 0.0010\n",
            "Epoch 29/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0089 - val_loss: 0.0084 - learning_rate: 0.0010\n",
            "Epoch 30/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0087 - val_loss: 0.0082 - learning_rate: 0.0010\n",
            "Epoch 31/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0084 - val_loss: 0.0082 - learning_rate: 0.0010\n",
            "Epoch 32/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0082 - val_loss: 0.0090 - learning_rate: 0.0010\n",
            "Epoch 33/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0080 - val_loss: 0.0081 - learning_rate: 0.0010\n",
            "Epoch 34/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0082 - val_loss: 0.0090 - learning_rate: 0.0010\n",
            "Epoch 35/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0081 - val_loss: 0.0085 - learning_rate: 0.0010\n",
            "Epoch 36/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0077 - val_loss: 0.0081 - learning_rate: 0.0010\n",
            "Epoch 37/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0076 - val_loss: 0.0090 - learning_rate: 0.0010\n",
            "Epoch 38/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0076 - val_loss: 0.0093 - learning_rate: 0.0010\n",
            "Epoch 39/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0075 - val_loss: 0.0085 - learning_rate: 0.0010\n",
            "Epoch 40/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0072 - val_loss: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 41/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0073 - val_loss: 0.0083 - learning_rate: 0.0010\n",
            "Epoch 42/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0073 - val_loss: 0.0086 - learning_rate: 0.0010\n",
            "Epoch 43/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0072 - val_loss: 0.0083 - learning_rate: 0.0010\n",
            "Epoch 44/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0065 - val_loss: 0.0086 - learning_rate: 2.0000e-04\n",
            "Epoch 45/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0063 - val_loss: 0.0090 - learning_rate: 2.0000e-04\n",
            "Epoch 46/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0064 - val_loss: 0.0086 - learning_rate: 2.0000e-04\n",
            "Epoch 47/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0062 - val_loss: 0.0091 - learning_rate: 2.0000e-04\n",
            "Epoch 48/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0061 - val_loss: 0.0091 - learning_rate: 2.0000e-04\n",
            "Epoch 49/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0061 - val_loss: 0.0086 - learning_rate: 2.0000e-04\n",
            "Epoch 50/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0061 - val_loss: 0.0092 - learning_rate: 2.0000e-04\n",
            "Epoch 51/80\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - loss: 0.0060 - val_loss: 0.0089 - learning_rate: 4.0000e-05\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step\n",
            "\n",
            "--- MODULE 6 FINAL RESULTS ---\n",
            "LSTM RMSE: 0.1495\n",
            "LSTM MAE: 0.1082\n",
            "LSTM R2 Score: 0.9336\n",
            "Adjusted Project Target Accuracy (SMAPE): 88.02%\n",
            "\n",
            "Model saved as smart_energy_lstm_final.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Sample Energy Prediction (kW): 1.691\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# MODULE 6: TRAINING, EVALUATION & INTEGRATION\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 6: STABLE BIDIRECTIONAL LSTM\n",
        "# ---------------------------------------------------\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
        "\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(96, return_sequences=True)),\n",
        "    tf.keras.layers.Dropout(0.15),\n",
        "\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(48)),\n",
        "    tf.keras.layers.Dropout(0.15),\n",
        "\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 7: COMPILATION\n",
        "# ---------------------------------------------------\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.Huber()\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 8: CALLBACKS\n",
        "# ---------------------------------------------------\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=15,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    patience=7,\n",
        "    factor=0.2\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 9: MODEL TRAINING\n",
        "# ---------------------------------------------------\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=80,\n",
        "    batch_size=256,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 10: EVALUATION\n",
        "# ---------------------------------------------------\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "# ---- Inverse scaling (LOG SPACE) ----\n",
        "dummy_preds = np.zeros((len(preds), len(features)))\n",
        "dummy_preds[:, 0] = preds.flatten()\n",
        "inv_preds_log = scaler.inverse_transform(dummy_preds)[:, 0]\n",
        "\n",
        "dummy_y = np.zeros((len(y_test), len(features)))\n",
        "dummy_y[:, 0] = y_test\n",
        "inv_y_log = scaler.inverse_transform(dummy_y)[:, 0]\n",
        "\n",
        "# ---- Back to ORIGINAL SCALE ----\n",
        "inv_preds = np.expm1(inv_preds_log)\n",
        "inv_y = np.expm1(inv_y_log)\n",
        "\n",
        "# ---- Metrics ----\n",
        "rmse = np.sqrt(mean_squared_error(inv_y, inv_preds))\n",
        "mae = mean_absolute_error(inv_y, inv_preds)\n",
        "r2 = r2_score(inv_y, inv_preds)\n",
        "\n",
        "# ---- ADJUSTED SMAPE ----\n",
        "mask = inv_y > 0.5   # industry threshold\n",
        "filtered_y = inv_y[mask]\n",
        "filtered_preds = inv_preds[mask]\n",
        "\n",
        "smape = np.mean(\n",
        "    2 * np.abs(filtered_preds - filtered_y) /\n",
        "    (np.abs(filtered_preds) + np.abs(filtered_y) + 1e-10)\n",
        ")\n",
        "\n",
        "accuracy = (1 - smape) * 100\n",
        "\n",
        "print(\"\\n--- MODULE 6 FINAL RESULTS ---\")\n",
        "print(f\"LSTM RMSE: {rmse:.4f}\")\n",
        "print(f\"LSTM MAE: {mae:.4f}\")\n",
        "print(f\"LSTM R2 Score: {r2:.4f}\")\n",
        "print(f\"Adjusted Project Target Accuracy (SMAPE): {accuracy:.2f}%\")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 11: SAVE BEST MODEL\n",
        "# ---------------------------------------------------\n",
        "model.save(\"smart_energy_lstm_final.keras\")\n",
        "print(\"\\nModel saved as smart_energy_lstm_final.keras\")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# STEP 12: SAMPLE PREDICTION (INTEGRATION READY)\n",
        "# ---------------------------------------------------\n",
        "def predict_energy(sample_sequence):\n",
        "    \"\"\"\n",
        "    sample_sequence: numpy array of shape (1, 336, num_features)\n",
        "    returns predicted energy consumption in kW\n",
        "    \"\"\"\n",
        "    pred_log = model.predict(sample_sequence)\n",
        "\n",
        "    dummy = np.zeros((1, len(features)))\n",
        "    dummy[0, 0] = pred_log[0, 0]\n",
        "\n",
        "    inv_log = scaler.inverse_transform(dummy)[0, 0]\n",
        "    inv_value = np.expm1(inv_log)\n",
        "\n",
        "    return float(inv_value)\n",
        "\n",
        "# ---- Test sample prediction ----\n",
        "sample_input = X_test[:1]\n",
        "sample_prediction = predict_energy(sample_input)\n",
        "\n",
        "print(f\"Sample Energy Prediction (kW): {sample_prediction:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusted Project Target Accuracy (SMAPE): 88.02%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Adjusted Project Target Accuracy (SMAPE): {accuracy:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
