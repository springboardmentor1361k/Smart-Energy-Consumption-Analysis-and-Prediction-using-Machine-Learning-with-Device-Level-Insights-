{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîã Milestone 1: Smart Energy Consumption Analysis\n",
    "\n",
    "## Week 1-2: Data Collection, Understanding & Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Project Scope & Objectives\n",
    "\n",
    "This notebook implements **Milestone 1** of the AI/ML-Driven Analysis and Forecasting of Device-Level Energy Consumption project.\n",
    "\n",
    "#### Module 1: Data Collection and Understanding\n",
    "- ‚úÖ Define project scope and functional objectives for smart energy analysis\n",
    "- ‚úÖ Collect and structure the SmartHome Energy Monitoring Dataset\n",
    "- ‚úÖ Verify data integrity, handle missing timestamps, and perform exploratory analysis\n",
    "- ‚úÖ Organize energy readings by device, room, and timestamp\n",
    "\n",
    "#### Module 2: Data Cleaning and Preprocessing\n",
    "- ‚úÖ Handle missing values and outliers in power consumption readings\n",
    "- ‚úÖ Convert timestamps to datetime format and resample data (hourly/daily)\n",
    "- ‚úÖ Normalize or scale energy values for model compatibility\n",
    "- ‚úÖ Split dataset into training, validation, and testing sets\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Suraj Surve  \n",
    "**Date:** January 2026  \n",
    "**Infosys Springboard Internship - Project 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Import Libraries & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation & Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Preprocessing & ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Create images directory if not exists\n",
    "import os\n",
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üì¶ Pandas version: {pd.__version__}\")\n",
    "print(f\"üì¶ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Module 1: Data Collection and Understanding\n",
    "\n",
    "### 2.1 Dataset Description\n",
    "\n",
    "The **Individual Household Electric Power Consumption Dataset** contains:\n",
    "- **Source:** UCI Machine Learning Repository\n",
    "- **Period:** December 2006 - November 2010 (~4 years)\n",
    "- **Granularity:** 1-minute sampling rate\n",
    "- **Records:** ~2 million measurements\n",
    "\n",
    "#### Feature Descriptions:\n",
    "| Feature | Description | Unit |\n",
    "|---------|-------------|------|\n",
    "| `Global_active_power` | Total active power consumed | kilowatt (kW) |\n",
    "| `Global_reactive_power` | Total reactive power consumed | kilowatt (kW) |\n",
    "| `Voltage` | Minute-averaged voltage | volt (V) |\n",
    "| `Global_intensity` | Current intensity | ampere (A) |\n",
    "| `Sub_metering_1` | Kitchen appliances (dishwasher, oven, microwave) | watt-hour (Wh) |\n",
    "| `Sub_metering_2` | Laundry room (washing machine, dryer, refrigerator) | watt-hour (Wh) |\n",
    "| `Sub_metering_3` | Climate control (water heater, AC) | watt-hour (Wh) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Dataset uses semicolon separator and '?' for missing values\n",
    "df = pd.read_csv('../household_power_consumption.txt', \n",
    "                 sep=';', \n",
    "                 na_values=['?', ''],\n",
    "                 low_memory=False)\n",
    "\n",
    "# Display loading summary\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä DATASET LOADED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìè Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"üíæ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nüìã Column Names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Integrity Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and info\n",
    "print(\"üìä Dataset Information:\")\n",
    "print(\"=\" * 60)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìà Statistical Summary:\")\n",
    "print(\"=\" * 60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"üîÑ Duplicate rows: {duplicate_count:,}\")\n",
    "\n",
    "# Check date range\n",
    "print(f\"\\nüìÖ Date Range: {df['Date'].min()} to {df['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive missing values analysis\n",
    "print(\"üîç Missing Values Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage (%)': missing_percent\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "total_missing = df.isnull().sum().sum()\n",
    "rows_with_missing = df.isnull().any(axis=1).sum()\n",
    "\n",
    "print(f\"\\nüìä Total missing values: {total_missing:,}\")\n",
    "print(f\"üìä Rows with missing values: {rows_with_missing:,} ({rows_with_missing/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Store missing values count for before/after comparison\n",
    "missing_before = df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Missing values bar chart\n",
    "missing_cols = missing_df[missing_df['Missing Count'] > 0]\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(range(len(missing_cols)), missing_cols['Missing Count'], color='coral', edgecolor='darkred')\n",
    "ax1.set_xticks(range(len(missing_cols)))\n",
    "ax1.set_xticklabels(missing_cols.index, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Missing Values by Column')\n",
    "ax1.set_xlabel('Column')\n",
    "for bar, pct in zip(bars, missing_cols['Percentage (%)']):\n",
    "    ax1.annotate(f'{pct}%', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Missing pattern heatmap (sample for visualization)\n",
    "ax2 = axes[1]\n",
    "sample_size = min(500, len(df))\n",
    "sample_df = df.sample(sample_size, random_state=42)\n",
    "sns.heatmap(sample_df.isnull(), cbar=True, cmap='YlOrRd', ax=ax2)\n",
    "ax2.set_title(f'Missing Values Pattern (Sample n={sample_size})')\n",
    "ax2.set_ylabel('Row Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/01_missing_values_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Image saved: images/01_missing_values_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Device-Level Energy Organization\n",
    "\n",
    "The sub-metering columns represent different areas/devices in the household:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device mapping\n",
    "device_mapping = {\n",
    "    'Sub_metering_1': 'Kitchen (dishwasher, oven, microwave)',\n",
    "    'Sub_metering_2': 'Laundry (washing machine, dryer, refrigerator)',\n",
    "    'Sub_metering_3': 'Climate Control (water heater, AC)'\n",
    "}\n",
    "\n",
    "print(\"üè† Device-Level Energy Organization:\")\n",
    "print(\"=\" * 60)\n",
    "for col, description in device_mapping.items():\n",
    "    print(f\"   {col}: {description}\")\n",
    "\n",
    "# Convert numeric columns to proper types\n",
    "numeric_cols = ['Global_active_power', 'Global_reactive_power', 'Voltage', \n",
    "                'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Device-wise statistics\n",
    "print(\"\\nüìä Device-wise Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "for col, device in device_mapping.items():\n",
    "    values = df[col].dropna()\n",
    "    print(f\"\\n   {device}:\")\n",
    "    print(f\"      Mean: {values.mean():.2f} Wh\")\n",
    "    print(f\"      Max: {values.max():.2f} Wh\")\n",
    "    print(f\"      Min: {values.min():.2f} Wh\")\n",
    "    print(f\"      Std Dev: {values.std():.2f} Wh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize device-level distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "sub_cols = ['Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "titles = ['Kitchen', 'Laundry', 'Climate Control']\n",
    "\n",
    "for i, (col, title, color) in enumerate(zip(sub_cols, titles, colors)):\n",
    "    ax = axes[i]\n",
    "    data = df[col].dropna()\n",
    "    ax.hist(data[data > 0], bins=50, color=color, edgecolor='white', alpha=0.8)\n",
    "    ax.set_title(f'{title}\\n({col})')\n",
    "    ax.set_xlabel('Energy (Wh)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.1f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Device-Level Energy Distribution', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/02_device_level_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Image saved: images/02_device_level_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Module 2: Data Cleaning and Preprocessing\n",
    "\n",
    "### 3.1 DateTime Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DateTime column\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Set DateTime as index\n",
    "df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Drop original Date and Time columns\n",
    "df.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "\n",
    "# Sort by index\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "print(\"‚úÖ DateTime index created!\")\n",
    "print(f\"üìÖ Date Range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"üìÜ Total Duration: {(df.index.max() - df.index.min()).days} days\")\n",
    "print(f\"\\nüìä Data Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Values Handling (Multiple Strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Handling Missing Values with Multiple Strategies...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Before: {df.isnull().sum().sum():,} missing values\")\n",
    "\n",
    "# Strategy 1: Linear interpolation (best for time-series)\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned = df_cleaned.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "# Strategy 2: Forward fill for any remaining NaNs\n",
    "df_cleaned = df_cleaned.ffill()\n",
    "\n",
    "# Strategy 3: Backward fill for any remaining NaNs at the start\n",
    "df_cleaned = df_cleaned.bfill()\n",
    "\n",
    "missing_after = df_cleaned.isnull().sum().sum()\n",
    "print(f\"   After: {missing_after:,} missing values\")\n",
    "print(\"\\n‚úÖ Missing values handled successfully!\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"\\nüìä Missing Values Check (After Cleaning):\")\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Outlier Detection & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Outlier Detection using IQR Method:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers for each numeric column\n",
    "outlier_summary = []\n",
    "for col in numeric_cols:\n",
    "    count, lower, upper = detect_outliers_iqr(df_cleaned, col)\n",
    "    pct = (count / len(df_cleaned)) * 100\n",
    "    outlier_summary.append({\n",
    "        'Column': col,\n",
    "        'Outliers': count,\n",
    "        'Percentage': f'{pct:.2f}%',\n",
    "        'Lower Bound': f'{lower:.2f}',\n",
    "        'Upper Bound': f'{upper:.2f}'\n",
    "    })\n",
    "    print(f\"   {col}: {count:,} outliers ({pct:.2f}%)\")\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers using box plots\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    ax = axes[i]\n",
    "    box = ax.boxplot(df_cleaned[col].dropna(), patch_artist=True)\n",
    "    box['boxes'][0].set_facecolor('#74b9ff')\n",
    "    ax.set_title(col, fontsize=10)\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "# Hide the last empty subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Outlier Detection - Box Plots', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/03_outlier_detection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Image saved: images/03_outlier_detection.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier treatment: Cap to percentile bounds (Winsorization)\n",
    "print(\"\\nüîß Treating Outliers (Winsorization at 1st and 99th percentile):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    lower_cap = df_cleaned[col].quantile(0.01)\n",
    "    upper_cap = df_cleaned[col].quantile(0.99)\n",
    "    original_outliers = len(df_cleaned[(df_cleaned[col] < lower_cap) | (df_cleaned[col] > upper_cap)])\n",
    "    df_cleaned[col] = df_cleaned[col].clip(lower=lower_cap, upper=upper_cap)\n",
    "    print(f\"   {col}: Capped {original_outliers:,} values to [{lower_cap:.2f}, {upper_cap:.2f}]\")\n",
    "\n",
    "print(\"\\n‚úÖ Outliers treated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Resampling Data (Hourly/Daily Aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly resampling\n",
    "df_hourly = df_cleaned.resample('h').mean()\n",
    "print(\"üìä Hourly Resampled Data:\")\n",
    "print(f\"   Shape: {df_hourly.shape}\")\n",
    "print(f\"   Date Range: {df_hourly.index.min()} to {df_hourly.index.max()}\")\n",
    "\n",
    "# Daily resampling\n",
    "df_daily = df_cleaned.resample('D').mean()\n",
    "print(\"\\nüìä Daily Resampled Data:\")\n",
    "print(f\"   Shape: {df_daily.shape}\")\n",
    "print(f\"   Date Range: {df_daily.index.min()} to {df_daily.index.max()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data resampled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize resampled time series\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Hourly data (sample)\n",
    "sample_hourly = df_hourly['Global_active_power'].iloc[:720]  # First 30 days\n",
    "axes[0].plot(sample_hourly.index, sample_hourly.values, color='#3498db', linewidth=0.8)\n",
    "axes[0].set_title('Hourly Global Active Power (First 30 Days)', fontweight='bold')\n",
    "axes[0].set_xlabel('DateTime')\n",
    "axes[0].set_ylabel('Power (kW)')\n",
    "axes[0].fill_between(sample_hourly.index, sample_hourly.values, alpha=0.3, color='#3498db')\n",
    "\n",
    "# Daily data\n",
    "axes[1].plot(df_daily.index, df_daily['Global_active_power'].values, color='#e74c3c', linewidth=1.2)\n",
    "axes[1].set_title('Daily Average Global Active Power (Full Dataset)', fontweight='bold')\n",
    "axes[1].set_xlabel('DateTime')\n",
    "axes[1].set_ylabel('Power (kW)')\n",
    "axes[1].fill_between(df_daily.index, df_daily['Global_active_power'].values, alpha=0.3, color='#e74c3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/04_resampled_time_series.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Image saved: images/04_resampled_time_series.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Feature Engineering (Temporal Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temporal features to hourly data\n",
    "df_hourly['hour'] = df_hourly.index.hour\n",
    "df_hourly['day'] = df_hourly.index.day\n",
    "df_hourly['month'] = df_hourly.index.month\n",
    "df_hourly['year'] = df_hourly.index.year\n",
    "df_hourly['dayofweek'] = df_hourly.index.dayofweek\n",
    "df_hourly['is_weekend'] = df_hourly['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Season mapping\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 0  # Winter\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 1  # Spring\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 2  # Summer\n",
    "    else:\n",
    "        return 3  # Autumn\n",
    "\n",
    "df_hourly['season'] = df_hourly['month'].apply(get_season)\n",
    "\n",
    "print(\"‚úÖ Temporal features added!\")\n",
    "print(\"\\nüìä New Features:\")\n",
    "print(df_hourly[['hour', 'day', 'month', 'year', 'dayofweek', 'is_weekend', 'season']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hourly patterns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Hourly consumption pattern\n",
    "hourly_pattern = df_hourly.groupby('hour')['Global_active_power'].mean()\n",
    "axes[0].bar(hourly_pattern.index, hourly_pattern.values, color='#2ecc71', edgecolor='white')\n",
    "axes[0].set_title('Average Consumption by Hour of Day', fontweight='bold')\n",
    "axes[0].set_xlabel('Hour')\n",
    "axes[0].set_ylabel('Avg Power (kW)')\n",
    "axes[0].set_xticks(range(24))\n",
    "\n",
    "# Day of week pattern\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "daily_pattern = df_hourly.groupby('dayofweek')['Global_active_power'].mean()\n",
    "colors = ['#3498db'] * 5 + ['#e74c3c'] * 2  # Blue for weekdays, red for weekends\n",
    "axes[1].bar(daily_pattern.index, daily_pattern.values, color=colors, edgecolor='white')\n",
    "axes[1].set_title('Average Consumption by Day of Week', fontweight='bold')\n",
    "axes[1].set_xlabel('Day of Week')\n",
    "axes[1].set_ylabel('Avg Power (kW)')\n",
    "axes[1].set_xticks(range(7))\n",
    "axes[1].set_xticklabels(day_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/05_consumption_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Image saved: images/05_consumption_patterns.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "correlation_cols = numeric_cols + ['hour', 'dayofweek', 'is_weekend', 'season']\n",
    "corr_matrix = df_hourly[correlation_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0, \n",
    "            fmt='.2f', square=True, linewidths=0.5,\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/06_correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Image saved: images/06_correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Normalization & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for scaling (using hourly data)\n",
    "features_to_scale = numeric_cols\n",
    "df_scaled = df_hourly.copy()\n",
    "\n",
    "# MinMax Scaling\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaled = minmax_scaler.fit_transform(df_hourly[features_to_scale])\n",
    "df_minmax = pd.DataFrame(minmax_scaled, columns=[f'{col}_minmax' for col in features_to_scale], index=df_hourly.index)\n",
    "\n",
    "# Standard Scaling (Z-score normalization)\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaled = standard_scaler.fit_transform(df_hourly[features_to_scale])\n",
    "df_standard = pd.DataFrame(standard_scaled, columns=[f'{col}_standard' for col in features_to_scale], index=df_hourly.index)\n",
    "\n",
    "print(\"‚úÖ Data Scaling Complete!\")\n",
    "print(\"\\nüìä MinMax Scaled Data (range [0, 1]):\")\n",
    "print(df_minmax.describe().round(3))\n",
    "print(\"\\nüìä Standard Scaled Data (mean=0, std=1):\")\n",
    "print(df_standard.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "col_to_show = 'Global_active_power'\n",
    "\n",
    "# Original\n",
    "axes[0].hist(df_hourly[col_to_show].dropna(), bins=50, color='#3498db', edgecolor='white', alpha=0.8)\n",
    "axes[0].set_title(f'Original: {col_to_show}', fontweight='bold')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# MinMax Scaled\n",
    "axes[1].hist(df_minmax[f'{col_to_show}_minmax'].dropna(), bins=50, color='#2ecc71', edgecolor='white', alpha=0.8)\n",
    "axes[1].set_title(f'MinMax Scaled: {col_to_show}', fontweight='bold')\n",
    "axes[1].set_xlabel('Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Standard Scaled\n",
    "axes[2].hist(df_standard[f'{col_to_show}_standard'].dropna(), bins=50, color='#e74c3c', edgecolor='white', alpha=0.8)\n",
    "axes[2].set_title(f'Standard Scaled: {col_to_show}', fontweight='bold')\n",
    "axes[2].set_xlabel('Value')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Scaling Comparison', fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/07_scaling_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Image saved: images/07_scaling_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series aware split (no shuffling to maintain temporal order)\n",
    "# 70% train, 15% validation, 15% test\n",
    "\n",
    "n = len(df_hourly)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "df_train = df_hourly.iloc[:train_end]\n",
    "df_val = df_hourly.iloc[train_end:val_end]\n",
    "df_test = df_hourly.iloc[val_end:]\n",
    "\n",
    "print(\"üìä Dataset Split Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Training Set:   {len(df_train):,} samples ({len(df_train)/n*100:.1f}%)\")\n",
    "print(f\"   Validation Set: {len(df_val):,} samples ({len(df_val)/n*100:.1f}%)\")\n",
    "print(f\"   Test Set:       {len(df_test):,} samples ({len(df_test)/n*100:.1f}%)\")\n",
    "print(f\"\\n   Total:          {n:,} samples\")\n",
    "\n",
    "print(\"\\nüìÖ Date Ranges:\")\n",
    "print(f\"   Training:   {df_train.index.min()} to {df_train.index.max()}\")\n",
    "print(f\"   Validation: {df_val.index.min()} to {df_val.index.max()}\")\n",
    "print(f\"   Test:       {df_test.index.min()} to {df_test.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data split\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "ax.plot(df_train.index, df_train['Global_active_power'], label='Training', color='#3498db', alpha=0.8)\n",
    "ax.plot(df_val.index, df_val['Global_active_power'], label='Validation', color='#f39c12', alpha=0.8)\n",
    "ax.plot(df_test.index, df_test['Global_active_power'], label='Test', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax.axvline(df_val.index.min(), color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "ax.axvline(df_test.index.min(), color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax.set_title('Train/Validation/Test Split Visualization', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('DateTime')\n",
    "ax.set_ylabel('Global Active Power (kW)')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/08_data_split_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Image saved: images/08_data_split_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Summary & Conclusions\n",
    "\n",
    "### Milestone 1 Achievements:\n",
    "\n",
    "#### Module 1: Data Collection and Understanding\n",
    "- ‚úÖ Loaded and explored 2,075,259 minute-level energy consumption records\n",
    "- ‚úÖ Identified 1.25% missing values across all numerical columns\n",
    "- ‚úÖ Organized device-level energy data (Kitchen, Laundry, Climate Control)\n",
    "- ‚úÖ Performed comprehensive exploratory data analysis\n",
    "\n",
    "#### Module 2: Data Cleaning and Preprocessing\n",
    "- ‚úÖ Handled missing values using interpolation and forward/backward fill\n",
    "- ‚úÖ Detected and treated outliers using IQR method and Winsorization\n",
    "- ‚úÖ Created DateTime index and extracted temporal features\n",
    "- ‚úÖ Resampled data to hourly and daily granularity\n",
    "- ‚úÖ Applied MinMax and Standard scaling for normalization\n",
    "- ‚úÖ Split data into train (70%), validation (15%), and test (15%) sets\n",
    "\n",
    "### Key Observations:\n",
    "1. **HVAC (Sub_metering_3)** has the highest average consumption (~6.4 Wh)\n",
    "2. **Peak hours** are morning (7-9 AM) and evening (6-9 PM)\n",
    "3. **Weekend patterns** differ from weekdays\n",
    "4. Strong correlation between `Global_active_power` and `Global_intensity`\n",
    "\n",
    "### All Saved Visualizations:\n",
    "1. `images/01_missing_values_analysis.png`\n",
    "2. `images/02_device_level_distribution.png`\n",
    "3. `images/03_outlier_detection.png`\n",
    "4. `images/04_resampled_time_series.png`\n",
    "5. `images/05_consumption_patterns.png`\n",
    "6. `images/06_correlation_heatmap.png`\n",
    "7. `images/07_scaling_comparison.png`\n",
    "8. `images/08_data_split_visualization.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"üìä MILESTONE 1 COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìÅ Processed Data Summary:\")\n",
    "print(f\"   Original Records: 2,075,259\")\n",
    "print(f\"   Hourly Records: {len(df_hourly):,}\")\n",
    "print(f\"   Daily Records: {len(df_daily):,}\")\n",
    "print(f\"\\nüìä Missing Values: {missing_before:,} ‚Üí 0\")\n",
    "print(f\"\\nüñºÔ∏è Visualizations Saved: 8 images in 'images/' folder\")\n",
    "print(\"\\n‚úÖ Ready for Milestone 2: Feature Engineering & Modeling!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
