{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Milestone 3: LSTM Model Development and Evaluation\n",
                "\n",
                "## Objective\n",
                "Develop an LSTM model to predict energy consumption using a sliding window approach. Evaluate the model against the baseline Linear Regression model and prepare the best model for deployment.\n",
                "\n",
                "## Steps\n",
                "1. **Data Loading & Preparation**: Load preprocessed data, scale, and create sequences.\n",
                "2. **LSTM Model Development**: Build, train, and save the LSTM model.\n",
                "3. **Evaluation**: Compare LSTM with Baseline Linear Regression.\n",
                "4. **Prediction Function**: Create a function for Flask integration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
                "from tensorflow.keras.callbacks import EarlyStopping\n",
                "import pickle\n",
                "import os\n",
                "\n",
                "# Set seeds for reproducibility\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "print(\"Libraries imported successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the preprocessed data\n",
                "data_path = '../milestone2/feature_engineered_data.csv'\n",
                "df = pd.read_csv(data_path)\n",
                "\n",
                "# Convert timestamp to datetime if not already\n",
                "# Assuming 'Datetime' is the column name, adjust if necessary after inspection\n",
                "if 'Datetime' in df.columns:\n",
                "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
                "    df.set_index('Datetime', inplace=True)\n",
                "\n",
                "print(\"Data loaded. Shape:\", df.shape)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the target variable\n",
                "target_col = 'Global_active_power'  # Adjust if column name is different\n",
                "\n",
                "# Ensure the data is sorted by time\n",
                "df.sort_index(inplace=True)\n",
                "\n",
                "# Use only the target column for sequence generation (univariate time series for now)\n",
                "# If multivariate is needed, include other features here\n",
                "data = df[[target_col]].values\n",
                "\n",
                "# Scaling\n",
                "scaler = MinMaxScaler(feature_range=(0, 1))\n",
                "scaled_data = scaler.fit_transform(data)\n",
                "\n",
                "print(\"Data scaled.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sequences(data, seq_length):\n",
                "    X, y = [], []\n",
                "    for i in range(len(data) - seq_length):\n",
                "        X.append(data[i:i+seq_length])\n",
                "        y.append(data[i+seq_length])\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "SEQ_LENGTH = 24  # Past 24 hours\n",
                "\n",
                "X, y = create_sequences(scaled_data, SEQ_LENGTH)\n",
                "\n",
                "print(\"Sequences created.\")\n",
                "print(\"X shape:\", X.shape)\n",
                "print(\"y shape:\", y.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split into Train (70%), Validation (15%), Test (15%)\n",
                "train_size = int(len(X) * 0.70)\n",
                "val_size = int(len(X) * 0.15)\n",
                "test_size = len(X) - train_size - val_size\n",
                "\n",
                "X_train, y_train = X[:train_size], y[:train_size]\n",
                "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
                "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
                "\n",
                "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
                "print(\"Val shape:\", X_val.shape, y_val.shape)\n",
                "print(\"Test shape:\", X_test.shape, y_test.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. LSTM Model Development"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_lstm_model(input_shape):\n",
                "    model = Sequential([\n",
                "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
                "        Dropout(0.2),\n",
                "        LSTM(32, return_sequences=False),\n",
                "        Dense(1)\n",
                "    ])\n",
                "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
                "    return model\n",
                "\n",
                "input_shape = (X_train.shape[1], X_train.shape[2])\n",
                "model = build_lstm_model(input_shape)\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "epochs_list = [20, 40, 60]\n",
                "batch_sizes = [16, 32]\n",
                "best_val_loss = float('inf')\n",
                "best_model = None\n",
                "best_history = None\n",
                "best_params = {}\n",
                "\n",
                "# Early stopping\n",
                "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
                "\n",
                "# Grid search for epochs and batch size (simplified)\n",
                "# Note: In a real scenario, we might want to separate this loop or use a proper tuner.\n",
                "# For this task, we will train with one config and then maybe mention others or loop.\n",
                "# Let's stick to the prompt: \"Train model using Epochs: test with 20, 40, and 60. Batch sizes: 16 and 32.\"\n",
                "# To save time, I will iterate but update best model.\n",
                "\n",
                "for epochs in epochs_list:\n",
                "    for batch_size in batch_sizes:\n",
                "        print(f\"Training with params: epochs={epochs}, batch_size={batch_size}\")\n",
                "        \n",
                "        # Re-build model to start fresh\n",
                "        temp_model = build_lstm_model(input_shape)\n",
                "        \n",
                "        history = temp_model.fit(\n",
                "            X_train, y_train,\n",
                "            epochs=epochs,\n",
                "            batch_size=batch_size,\n",
                "            validation_data=(X_val, y_val),\n",
                "            callbacks=[early_stopping],\n",
                "            verbose=0  # Reduce output\n",
                "        )\n",
                "        \n",
                "        val_loss = min(history.history['val_loss'])\n",
                "        print(f\"Best Val Loss: {val_loss}\")\n",
                "        \n",
                "        if val_loss < best_val_loss:\n",
                "            best_val_loss = val_loss\n",
                "            best_model = temp_model\n",
                "            best_history = history\n",
                "            best_params = {'epochs': epochs, 'batch_size': batch_size}\n",
                "\n",
                "print(\"Best Params:\", best_params)\n",
                "print(\"Best Val Loss:\", best_val_loss)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training vs validation loss for the best model\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(best_history.history['loss'], label='Train Loss')\n",
                "plt.plot(best_history.history['val_loss'], label='Validation Loss')\n",
                "plt.title(f'Model Loss (Epochs={best_params[\"epochs\"]}, Batch={best_params[\"batch_size\"]})')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss (MSE)')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the best LSTM model\n",
                "best_model.save('lstm_energy_model.h5')\n",
                "print(\"LSTM model saved as 'lstm_energy_model.h5'.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluation & Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate LSTM on Test Set\n",
                "lstm_preds_scaled = best_model.predict(X_test)\n",
                "lstm_preds = scaler.inverse_transform(lstm_preds_scaled)\n",
                "y_test_actual = scaler.inverse_transform(y_test)\n",
                "\n",
                "lstm_mae = mean_absolute_error(y_test_actual, lstm_preds)\n",
                "lstm_rmse = np.sqrt(mean_squared_error(y_test_actual, lstm_preds))\n",
                "lstm_r2 = r2_score(y_test_actual, lstm_preds)\n",
                "\n",
                "print(\"LSTM Results:\")\n",
                "print(f\"MAE: {lstm_mae}\")\n",
                "print(f\"RMSE: {lstm_rmse}\")\n",
                "print(f\"R2: {lstm_r2}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Baseline Model\n",
                "baseline_model_path = '../milestone2/baseline_linear_regression.pkl'\n",
                "with open(baseline_model_path, 'rb') as f:\n",
                "    baseline_model = pickle.load(f)\n",
                "    \n",
                "# Check what the baseline model expects as input.\n",
                "# Typically, Linear Regression might expect features like [Lag_1, Lag_2, ...]. \n",
                "# We need to reconstruct the test set for the baseline model to be comparable.\n",
                "# Assuming the baseline used similar features or we use the 'feature_engineered_data.csv' directly.\n",
                "# Since the request says \"Evaluate both\", I'll assume we can use the same X_test if it was shaped correctly, \n",
                "# OR we need to use the original dataframe's test split features.\n",
                "\n",
                "# Let's check the columns of the original df to see what features were used.\n",
                "print(df.columns)\n",
                "\n",
                "# Attempt to load X_test and y_test consistent with Milestone 2 if possible.\n",
                "# For now, as a placeholder, I will assume the baseline model can predict on the same features \n",
                "# OR I will assume we have to create the features again.\n",
                "\n",
                "# IMPORTANT: The LSTM uses just the 'Global_active_power' sequence.\n",
                "# The Linear Regression likely used 'Lag_1', 'Lag_2', 'Rolling_Mean', etc.\n",
                "# We need to prepare data for Baseline.\n",
                "# I'll try to find common indices in X_test (which corresponds to timestamps) and extract features for those.\n",
                "\n",
                "# Reconstruct timestamps for X_test\n",
                "# Timestamps for y_test are from (train_size + val_size + seq_length) to end\n",
                "test_start_idx = train_size + val_size + SEQ_LENGTH\n",
                "test_timestamps = df.index[test_start_idx : test_start_idx + len(y_test)]\n",
                "\n",
                "# Extract features for these timestamps. \n",
                "# We need to drop 'Global_active_power' and 'Datetime' if they are in features.\n",
                "feature_cols = [c for c in df.columns if c not in ['Global_active_power', 'Datetime']]\n",
                "X_test_baseline = df.loc[test_timestamps, feature_cols]\n",
                "y_test_baseline = df.loc[test_timestamps, 'Global_active_power']\n",
                "\n",
                "# Handle missing features if any (though preprocessing should have handled it)\n",
                "X_test_baseline.fillna(0, inplace=True)\n",
                "\n",
                "try:\n",
                "    baseline_preds = baseline_model.predict(X_test_baseline)\n",
                "    \n",
                "    baseline_mae = mean_absolute_error(y_test_baseline, baseline_preds)\n",
                "    baseline_rmse = np.sqrt(mean_squared_error(y_test_baseline, baseline_preds))\n",
                "    baseline_r2 = r2_score(y_test_baseline, baseline_preds)\n",
                "    \n",
                "    print(\"Baseline Results:\")\n",
                "    print(f\"MAE: {baseline_mae}\")\n",
                "    print(f\"RMSE: {baseline_rmse}\")\n",
                "    print(f\"R2: {baseline_r2}\")\n",
                "except Exception as e:\n",
                "    print(f\"Could not evaluate baseline model: {e}\")\n",
                "    baseline_mae, baseline_rmse, baseline_r2 = np.nan, np.nan, np.nan"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comparison Table\n",
                "results = {\n",
                "    'Model': ['Linear Regression', 'LSTM'],\n",
                "    'MAE': [baseline_mae, lstm_mae],\n",
                "    'RMSE': [baseline_rmse, lstm_rmse],\n",
                "    'R2': [baseline_r2, lstm_r2]\n",
                "}\n",
                "results_df = pd.DataFrame(results)\n",
                "print(results_df)\n",
                "\n",
                "# Select Best Model\n",
                "if lstm_rmse < baseline_rmse:\n",
                "    print(\"Best Model: LSTM\")\n",
                "    best_final_model = best_model\n",
                "    best_model_name = 'best_energy_model.h5'\n",
                "    best_final_model.save(best_model_name)\n",
                "else:\n",
                "    print(\"Best Model: Linear Regression\")\n",
                "    # Baseline is already saved as pkl, we can copy it or just reference it.\n",
                "    # Instruction says save as best_energy_model.pkl\n",
                "    best_model_name = 'best_energy_model.pkl'\n",
                "    with open(best_model_name, 'wb') as f:\n",
                "        pickle.dump(baseline_model, f)\n",
                "\n",
                "print(f\"Best model saved as {best_model_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Flask-Compatible Prediction Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_energy(input_sequence):\n",
                "    \"\"\"\n",
                "    input_sequence: Array-like of shape (24,) representing past 24 hours of energy data.\n",
                "    returns: Predicted energy value.\n",
                "    \"\"\"\n",
                "    # Ensure input is numpy array\n",
                "    input_seq = np.array(input_sequence)\n",
                "    \n",
                "    # Reshape for scalar\n",
                "    input_seq = input_seq.reshape(-1, 1)\n",
                "    \n",
                "    # Scale\n",
                "    # Note: We should use the scaler fitted on training data\n",
                "    scaled_seq = scaler.transform(input_seq)\n",
                "    \n",
                "    # Reshape for LSTM (1, 24, 1)\n",
                "    model_input = scaled_seq.reshape(1, SEQ_LENGTH, 1)\n",
                "    \n",
                "    # Predict\n",
                "    scaled_prediction = best_model.predict(model_input, verbose=0)\n",
                "    \n",
                "    # Inverse scale\n",
                "    prediction = scaler.inverse_transform(scaled_prediction)\n",
                "    \n",
                "    return prediction[0][0]\n",
                "\n",
                "# Test with a sample from test set\n",
                "sample_idx = 0\n",
                "sample_input = df['Global_active_power'].iloc[test_start_idx + sample_idx - SEQ_LENGTH : test_start_idx + sample_idx].values\n",
                "\n",
                "print(\"Sample Input Shape:\", sample_input.shape)\n",
                "predicted_value = predict_energy(sample_input)\n",
                "print(f\"Predicted Energy: {predicted_value}\")\n",
                "print(f\"Actual Energy: {y_test_actual[sample_idx][0]}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}